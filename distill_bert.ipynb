{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16c1ad4d-91ec-4bc1-b056-687a43645769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.7.0-cp39-cp39-manylinux_2_28_x86_64.whl (865.2 MB)\n",
      "\u001b[K     |█████████████▉                  | 372.9 MB 103.8 MB/s eta 0:00:05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |█████████████████████████▌      | 689.7 MB 93.5 MB/s eta 0:00:02     |█████████████████████▌          | 580.7 MB 93.1 MB/s eta 0:00:04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 865.2 MB 43 kB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting transformers\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.4 MB 99.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "\u001b[K     |████████████████████████████████| 491 kB 93.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-2.2.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.1 MB 94.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.5 MB 96.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy\n",
      "  Downloading numpy-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.5 MB 92.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.6.4.1\n",
      "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "\u001b[K     |██████████████████████          | 271.3 MB 101.9 MB/s eta 0:00:02"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 393.1 MB 49 kB/s /s eta 0:00:01\n",
      "\u001b[?25hCollecting networkx\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 54.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sympy>=1.13.3\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.3 MB 50.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting triton==3.3.0\n",
      "  Downloading triton-3.3.0-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 156.4 MB 110.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.3.0.4\n",
      "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "\u001b[K     |██████████████████████████▋     | 166.6 MB 107.2 MB/s eta 0:00:01"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 200.2 MB 201 kB/s \n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.6.77\n",
      "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[K     |████████████████████████████████| 89 kB 39.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.26.2\n",
      "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 201.3 MB 30 kB/s /s eta 0:00:01\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.7 MB 91.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.6.80\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.9 MB 68.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cusparselt-cu12==0.6.3\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 156.8 MB 98.4 MB/s eta 0:00:01    |███████████▎                    | 55.4 MB 71.9 MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.6.77\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "\u001b[K     |████████████████████████████████| 897 kB 92.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Downloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "\u001b[K     |████████████████████████████████| 194 kB 99.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.7.1.2\n",
      "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 158.2 MB 112.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.6.77\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 23.7 MB 91.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cufile-cu12==1.11.1.6\n",
      "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 93.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.5.4.2\n",
      "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 216.6 MB 126 kB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.7.77\n",
      "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 56.3 MB 95.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.10.0 in /home/user/miniconda/lib/python3.9/site-packages (from torch) (4.13.2)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17\n",
      "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "\u001b[K     |█████████████████▎              | 309.2 MB 113.2 MB/s eta 0:00:03"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 571.0 MB 48 kB/s /s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /home/user/miniconda/lib/python3.9/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/user/miniconda/lib/python3.9/site-packages (from triton==3.3.0->torch) (52.0.0.post20210125)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[K     |████████████████████████████████| 536 kB 105.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/user/miniconda/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Collecting safetensors>=0.4.3\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[K     |████████████████████████████████| 471 kB 104.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers<0.22,>=0.21\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 96.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /home/user/miniconda/lib/python3.9/site-packages (from transformers) (4.61.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/user/miniconda/lib/python3.9/site-packages (from transformers) (6.0.2)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0\n",
      "  Downloading huggingface_hub-0.31.2-py3-none-any.whl (484 kB)\n",
      "\u001b[K     |████████████████████████████████| 484 kB 101.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/user/miniconda/lib/python3.9/site-packages (from transformers) (25.0)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2024.11.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (780 kB)\n",
      "\u001b[K     |████████████████████████████████| 780 kB 99.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm>=4.27\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 34.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyarrow>=15.0.0\n",
      "  Downloading pyarrow-20.0.0-cp39-cp39-manylinux_2_28_x86_64.whl (42.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 42.3 MB 81.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multiprocess<0.70.17\n",
      "  Downloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 95.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\n",
      "\u001b[K     |████████████████████████████████| 193 kB 71.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dill<0.3.9,>=0.3.0\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[K     |████████████████████████████████| 116 kB 101.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec[http]<=2025.3.0,>=2023.1.0\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[K     |████████████████████████████████| 193 kB 95.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.11.18-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 94.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.4.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
      "\u001b[K     |████████████████████████████████| 216 kB 94.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /home/user/miniconda/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting propcache>=0.2.0\n",
      "  Downloading propcache-0.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209 kB)\n",
      "\u001b[K     |████████████████████████████████| 209 kB 92.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.6.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
      "\u001b[K     |████████████████████████████████| 288 kB 86.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.20.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (335 kB)\n",
      "\u001b[K     |████████████████████████████████| 335 kB 83.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting async-timeout<6.0,>=4.0\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/user/miniconda/lib/python3.9/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/user/miniconda/lib/python3.9/site-packages (from requests->transformers) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/user/miniconda/lib/python3.9/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/user/miniconda/lib/python3.9/site-packages (from requests->transformers) (3.4.2)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[K     |████████████████████████████████| 347 kB 99.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytz>=2020.1\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[K     |████████████████████████████████| 509 kB 98.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /home/user/miniconda/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/user/miniconda/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Collecting scipy>=1.6.0\n",
      "  Downloading scipy-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 38.6 MB 96.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib>=1.2.0\n",
      "  Downloading joblib-1.5.0-py3-none-any.whl (307 kB)\n",
      "\u001b[K     |████████████████████████████████| 307 kB 87.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /home/user/miniconda/lib/python3.9/site-packages (from jinja2->torch) (3.0.2)\n",
      "Installing collected packages: propcache, multidict, frozenlist, yarl, tqdm, nvidia-nvjitlink-cu12, fsspec, filelock, async-timeout, aiosignal, aiohappyeyeballs, tzdata, pytz, nvidia-cusparse-cu12, nvidia-cublas-cu12, numpy, mpmath, huggingface-hub, dill, aiohttp, xxhash, triton, tokenizers, threadpoolctl, sympy, scipy, safetensors, regex, pyarrow, pandas, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparselt-cu12, nvidia-cusolver-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, networkx, multiprocess, joblib, transformers, torch, scikit-learn, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.61.2\n",
      "    Uninstalling tqdm-4.61.2:\n",
      "      Successfully uninstalled tqdm-4.61.2\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 async-timeout-5.0.1 datasets-3.6.0 dill-0.3.8 filelock-3.18.0 frozenlist-1.6.0 fsspec-2025.3.0 huggingface-hub-0.31.2 joblib-1.5.0 mpmath-1.3.0 multidict-6.4.3 multiprocess-0.70.16 networkx-3.2.1 numpy-2.0.2 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 pandas-2.2.3 propcache-0.3.1 pyarrow-20.0.0 pytz-2025.2 regex-2024.11.6 safetensors-0.5.3 scikit-learn-1.6.1 scipy-1.13.1 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.21.1 torch-2.7.0 tqdm-4.67.1 transformers-4.51.3 triton-3.3.0 tzdata-2025.2 xxhash-3.5.0 yarl-1.20.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers datasets pandas scikit-learn numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d67370b-395a-49b9-a193-9f29d93213b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in /home/user/miniconda/lib/python3.9/site-packages (4.51.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/user/miniconda/lib/python3.9/site-packages (from transformers[torch]) (4.67.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/user/miniconda/lib/python3.9/site-packages (from transformers[torch]) (25.0)\n",
      "Requirement already satisfied: filelock in /home/user/miniconda/lib/python3.9/site-packages (from transformers[torch]) (3.18.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/user/miniconda/lib/python3.9/site-packages (from transformers[torch]) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/user/miniconda/lib/python3.9/site-packages (from transformers[torch]) (0.21.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/user/miniconda/lib/python3.9/site-packages (from transformers[torch]) (2.0.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/user/miniconda/lib/python3.9/site-packages (from transformers[torch]) (0.31.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/user/miniconda/lib/python3.9/site-packages (from transformers[torch]) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/user/miniconda/lib/python3.9/site-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/user/miniconda/lib/python3.9/site-packages (from transformers[torch]) (0.5.3)\n",
      "Requirement already satisfied: torch>=2.0 in /home/user/miniconda/lib/python3.9/site-packages (from transformers[torch]) (2.7.0)\n",
      "Collecting accelerate>=0.26.0\n",
      "  Downloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
      "\u001b[K     |████████████████████████████████| 362 kB 20.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: psutil in /home/user/miniconda/lib/python3.9/site-packages (from accelerate>=0.26.0->transformers[torch]) (7.0.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/user/miniconda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/user/miniconda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (4.13.2)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/user/miniconda/lib/python3.9/site-packages (from torch>=2.0->transformers[torch]) (1.11.1.6)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/user/miniconda/lib/python3.9/site-packages (from torch>=2.0->transformers[torch]) (0.6.3)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/user/miniconda/lib/python3.9/site-packages (from torch>=2.0->transformers[torch]) (3.3.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/user/miniconda/lib/python3.9/site-packages (from torch>=2.0->transformers[torch]) (11.3.0.4)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/user/miniconda/lib/python3.9/site-packages (from torch>=2.0->transformers[torch]) (1.14.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/user/miniconda/lib/python3.9/site-packages (from torch>=2.0->transformers[torch]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/user/miniconda/lib/python3.9/site-packages (from torch>=2.0->transformers[torch]) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/user/miniconda/lib/python3.9/site-packages (from torch>=2.0->transformers[torch]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/user/miniconda/lib/python3.9/site-packages (from torch>=2.0->transformers[torch]) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/user/miniconda/lib/python3.9/site-packages (from torch>=2.0->transformers[torch]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/user/miniconda/lib/python3.9/site-packages (from torch>=2.0->transformers[torch]) (11.7.1.2)\n",
      "Requirement already satisfied: networkx in /home/user/miniconda/lib/python3.9/site-packages (from torch>=2.0->transformers[torch]) (3.2.1)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/user/miniconda/lib/python3.9/site-packages (from torch>=2.0->transformers[torch]) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/user/miniconda/lib/python3.9/site-packages (from torch>=2.0->transformers[torch]) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/user/miniconda/lib/python3.9/site-packages (from torch>=2.0->transformers[torch]) (12.6.80)\n",
      "Requirement already satisfied: jinja2 in /home/user/miniconda/lib/python3.9/site-packages (from torch>=2.0->transformers[torch]) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/user/miniconda/lib/python3.9/site-packages (from torch>=2.0->transformers[torch]) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/user/miniconda/lib/python3.9/site-packages (from torch>=2.0->transformers[torch]) (2.26.2)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/user/miniconda/lib/python3.9/site-packages (from triton==3.3.0->torch>=2.0->transformers[torch]) (52.0.0.post20210125)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/user/miniconda/lib/python3.9/site-packages (from sympy>=1.13.3->torch>=2.0->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/user/miniconda/lib/python3.9/site-packages (from jinja2->torch>=2.0->transformers[torch]) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/user/miniconda/lib/python3.9/site-packages (from requests->transformers[torch]) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/user/miniconda/lib/python3.9/site-packages (from requests->transformers[torch]) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/user/miniconda/lib/python3.9/site-packages (from requests->transformers[torch]) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/user/miniconda/lib/python3.9/site-packages (from requests->transformers[torch]) (2.10)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a28ecbef-db78-4e5c-93a5-d73db0306d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d1aab1e8634422a997ecfde1a91f0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/2.45k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f75269f54c243d4b077159f48de893e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "classtrain.txt.parquet:   0%|          | 0.00/9.27M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee87c05f52944045bd8f7e79a4faeec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "democratic_only.dev.en.parquet:   0%|          | 0.00/237k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a8f2d9f1c814d70901a54d3cd0f0b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "republican_only.dev.en.parquet:   0%|          | 0.00/231k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d14a143a6bf48e6abee0b4264e54dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "democratic_only.test.en.parquet:   0%|          | 0.00/3.26M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d36007bea84d74bdfd64af9e878b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "republican_only.test.en.parquet:   0%|          | 0.00/3.12M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2476fe8a2b834b80bb669d4728b8bc3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/80000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e216646b82d47e799168d006d220d9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating eval split:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da396ceb1c084f8faa5cd19b6ef539eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/56000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 80000\n",
      "Test set size: 56000\n",
      "Initializing model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "495701ddb2fa4fd8acc6e6865fef0abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c378d4883c04b2ca9f2d4444052a2cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4ac41f5c52e4c938973afb684694081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c853fd0ee846448695b895e30937124e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee33d89764db4452ab8dfde1b04fd82d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating datasets...\n",
      "Initializing trainer...\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7500' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7500/7500 1:02:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Classification Report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.078500</td>\n",
       "      <td>0.061661</td>\n",
       "      <td>0.985732</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "           0       0.99      0.98      0.99     28000\n",
       "           1       0.98      0.99      0.99     28000\n",
       "\n",
       "    accuracy                           0.99     56000\n",
       "   macro avg       0.99      0.99      0.99     56000\n",
       "weighted avg       0.99      0.99      0.99     56000\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.036200</td>\n",
       "      <td>0.051941</td>\n",
       "      <td>0.989643</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "           0       0.99      0.99      0.99     28000\n",
       "           1       0.99      0.99      0.99     28000\n",
       "\n",
       "    accuracy                           0.99     56000\n",
       "   macro avg       0.99      0.99      0.99     56000\n",
       "weighted avg       0.99      0.99      0.99     56000\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>0.040840</td>\n",
       "      <td>0.991214</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "           0       0.99      0.99      0.99     28000\n",
       "           1       0.99      0.99      0.99     28000\n",
       "\n",
       "    accuracy                           0.99     56000\n",
       "   macro avg       0.99      0.99      0.99     56000\n",
       "weighted avg       0.99      0.99      0.99     56000\n",
       "</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    DistilBertForSequenceClassification,\n",
    "    DistilBertTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": encodings[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encodings[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, predictions),\n",
    "        'classification_report': classification_report(labels, predictions)\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load dataset from Hugging Face\n",
    "    print(\"Loading dataset...\")\n",
    "    dataset = load_dataset(\"thien/political\", \"classifier\")\n",
    "    train_dataset = dataset['train']\n",
    "    test_dataset = dataset['test']\n",
    "    \n",
    "    print(f\"Train set size: {len(train_dataset)}\")\n",
    "    print(f\"Test set size: {len(test_dataset)}\")\n",
    "    \n",
    "    # Convert text labels to numeric\n",
    "    label_map = {'democratic': 0, 'republican': 1}\n",
    "    train_labels = [label_map[label] for label in train_dataset['label']]\n",
    "    test_labels = [label_map[label] for label in test_dataset['label']]\n",
    "    \n",
    "    # Initialize model and tokenizer\n",
    "    print(\"Initializing model...\")\n",
    "    model_name = \"distilbert-base-uncased\"\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=2,\n",
    "        id2label={0: \"democratic\", 1: \"republican\"},\n",
    "        label2id={\"democratic\": 0, \"republican\": 1}\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Create datasets\n",
    "    print(\"Creating datasets...\")\n",
    "    train_dataset = TextClassificationDataset(\n",
    "        train_dataset['text'],\n",
    "        train_labels,\n",
    "        tokenizer\n",
    "    )\n",
    "\n",
    "    test_dataset = TextClassificationDataset(\n",
    "        test_dataset['text'],\n",
    "        test_labels,\n",
    "        tokenizer\n",
    "    )\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./distilbert_output\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        gradient_accumulation_steps=2,\n",
    "        learning_rate=2e-4,\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=100,\n",
    "        eval_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer\n",
    "    print(\"Initializing trainer...\")\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer)\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the model\n",
    "    print(\"Saving model...\")\n",
    "    trainer.save_model(\"./distilbert_final\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f3c6e82-7ca6-44c8-b8f1-b30801260c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_predict(model_path, test_texts, test_labels=None):\n",
    "    \"\"\"\n",
    "    Load a trained model and make predictions on new texts\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the saved model\n",
    "        test_texts (list): List of texts to predict\n",
    "        test_labels (list, optional): True labels if available\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (predictions, metrics) if test_labels provided, else just predictions\n",
    "    \"\"\"\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Create dataset\n",
    "    print(\"Creating dataset...\")\n",
    "    test_dataset = TextClassificationDataset(\n",
    "        test_texts,\n",
    "        test_labels if test_labels is not None else [0] * len(test_texts),\n",
    "        tokenizer\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer)\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    print(\"Making predictions...\")\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        'text': test_texts,\n",
    "        'predicted_label': pred_labels\n",
    "    })\n",
    "    \n",
    "    if test_labels is not None:\n",
    "        results_df['true_label'] = test_labels\n",
    "        accuracy = accuracy_score(test_labels, pred_labels)\n",
    "        print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(test_labels, pred_labels,\n",
    "                                 target_names=['Democratic (0)', 'Republican (1)']))\n",
    "    \n",
    "    # Save results\n",
    "    output_file = \"predictions.csv\"\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nResults saved to {output_file}\")\n",
    "    \n",
    "    return pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad9fa7b7-6577-4925-ba36-e9be49d1d1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n",
      "Creating dataset...\n",
      "Making predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_141/3514967018.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.9912\n",
      "\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Democratic (0)       0.99      0.99      0.99     28000\n",
      "Republican (1)       0.99      0.99      0.99     28000\n",
      "\n",
      "      accuracy                           0.99     56000\n",
      "     macro avg       0.99      0.99      0.99     56000\n",
      "  weighted avg       0.99      0.99      0.99     56000\n",
      "\n",
      "\n",
      "Results saved to predictions.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = {'democratic': 0, 'republican': 1}\n",
    "dataset = load_dataset(\"thien/political\", \"classifier\")\n",
    "test_dataset = dataset['test']\n",
    "test_labels = [label_map[label] for label in test_dataset['label']]\n",
    "load_and_predict('./distilbert_final', test_dataset['text'], test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93897cfc-2c2b-4405-a46a-d31dcdc8fc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_print_results(result_file=\"predictions.csv\"):\n",
    "    \"\"\"\n",
    "    Load and print results from a CSV file containing predictions\n",
    "    \n",
    "    Args:\n",
    "        result_file (str): Path to the CSV file containing results\n",
    "    \"\"\"\n",
    "    print(f\"\\nLoading results from {result_file}...\")\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.exists(result_file):\n",
    "        print(f\"Error: File {result_file} not found!\")\n",
    "        return\n",
    "    \n",
    "    # Load results\n",
    "    df = pd.read_csv(result_file)\n",
    "    \n",
    "    # Print basic statistics\n",
    "    print(\"\\nResults Summary:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Total samples: {len(df)}\")\n",
    "\n",
    "    if 'true_label' in df.columns and 'predicted_label' in df.columns:\n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(df['true_label'], df['predicted_label'])\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        # Print classification report\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(df['true_label'], df['predicted_label'],\n",
    "                                 target_names=['Democratic (0)', 'Republican (1)']))\n",
    "        \n",
    "        # Print confusion matrix\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        confusion = pd.crosstab(df['true_label'], df['predicted_label'],\n",
    "                              rownames=['True'], colnames=['Predicted'])\n",
    "        print(confusion)\n",
    "    \n",
    "    # Print sample predictions\n",
    "    print(\"\\nSample Predictions:\")\n",
    "    print(\"=\"*50)\n",
    "    sample_size = min(5, len(df))\n",
    "    for i in range(sample_size):\n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(f\"Text: {df['text'].iloc[i][:200]}...\")  # Print first 200 chars\n",
    "        if 'true_label' in df.columns:\n",
    "            print(f\"True Label: {df['true_label'].iloc[i]}\")\n",
    "        print(f\"Predicted Label: {df['predicted_label'].iloc[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d141e27-c6da-4334-a599-a40adcf1e006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading results from predictions.csv...\n",
      "\n",
      "Results Summary:\n",
      "==================================================\n",
      "Total samples: 56000\n",
      "Accuracy: 0.9912\n",
      "\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Democratic (0)       0.99      0.99      0.99     28000\n",
      "Republican (1)       0.99      0.99      0.99     28000\n",
      "\n",
      "      accuracy                           0.99     56000\n",
      "     macro avg       0.99      0.99      0.99     56000\n",
      "  weighted avg       0.99      0.99      0.99     56000\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "Predicted      0      1\n",
      "True                   \n",
      "0          27713    287\n",
      "1            205  27795\n",
      "\n",
      "Sample Predictions:\n",
      "==================================================\n",
      "\n",
      "Sample 1:\n",
      "Text: sad and disgusting farce dems are trying to pull....\n",
      "True Label: 0\n",
      "Predicted Label: 0\n",
      "\n",
      "Sample 2:\n",
      "Text: this must be said: sanders is the antidote to trump not clinton....\n",
      "True Label: 0\n",
      "Predicted Label: 0\n",
      "\n",
      "Sample 3:\n",
      "Text: fight the good fight tammy, america is rooting for you....\n",
      "True Label: 0\n",
      "Predicted Label: 0\n",
      "\n",
      "Sample 4:\n",
      "Text: thank you for listening to the concerns of your constituents, and for demonstrating that you embrace the importance of civil rights for all of us....\n",
      "True Label: 0\n",
      "Predicted Label: 0\n",
      "\n",
      "Sample 5:\n",
      "Text: obamacare would not pay for the treatment of may patients here in oregon with fatal illness because it was to expensive!!...\n",
      "True Label: 0\n",
      "Predicted Label: 0\n"
     ]
    }
   ],
   "source": [
    "load_and_print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7786d9-4b65-4d68-987c-5f3100737027",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
